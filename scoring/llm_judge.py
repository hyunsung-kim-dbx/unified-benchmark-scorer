"""
LLM-based judge for comparing query results.

Uses Claude or GPT to semantically compare expected vs actual results
and determine if they match.
"""

import json
import os
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from pathlib import Path


class LLMJudge(ABC):
    """Abstract base class for LLM judges."""

    def __init__(self, model: Optional[str] = None):
        self.model = model
        self._prompt_template = self._load_prompt_template()

    def _load_prompt_template(self) -> str:
        """Load the answer comparison prompt template."""
        prompt_path = Path(__file__).parent.parent / "prompts" / "answer_comparison.md"
        if prompt_path.exists():
            return prompt_path.read_text()
        return self._default_prompt()

    def _default_prompt(self) -> str:
        """Default prompt if file not found."""
        return """Compare the expected and actual query results.
Return JSON: {"passed": bool, "confidence": float, "failure_reason": str|null, "failure_category": str|null, "explanation": str}"""

    def compare(
        self,
        question: str,
        expected_sql: str,
        expected_result: Dict[str, Any],
        actual_sql: Optional[str],
        actual_result: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Compare expected vs actual results using LLM.

        Args:
            question: Original benchmark question
            expected_sql: Expected SQL query
            expected_result: Expected result from executing expected_sql
            actual_sql: SQL generated by system (may be None)
            actual_result: Result from system (may be None)

        Returns:
            {
                "passed": bool,
                "confidence": float,
                "failure_reason": str | None,
                "failure_category": str | None,
                "explanation": str
            }
        """
        # Handle case where actual result is missing
        if actual_result is None:
            return {
                "passed": False,
                "confidence": 1.0,
                "failure_reason": "System returned no result",
                "failure_category": "no_result",
                "explanation": "The system did not return any result to compare"
            }

        # Build prompt
        prompt = self._build_prompt(
            question=question,
            expected_sql=expected_sql,
            expected_result=expected_result,
            actual_sql=actual_sql,
            actual_result=actual_result
        )

        # Call LLM
        response = self._call_llm(prompt)

        # Parse response
        return self._parse_response(response)

    def _build_prompt(
        self,
        question: str,
        expected_sql: str,
        expected_result: Dict[str, Any],
        actual_sql: Optional[str],
        actual_result: Dict[str, Any],
    ) -> str:
        """Build the comparison prompt."""
        return self._prompt_template.format(
            question=question,
            expected_sql=expected_sql,
            expected_columns=expected_result.get("columns", []),
            expected_data=json.dumps(expected_result.get("data", [])[:50]),  # Limit rows
            expected_row_count=expected_result.get("row_count", 0),
            actual_sql=actual_sql or "N/A",
            actual_columns=actual_result.get("columns", []),
            actual_data=json.dumps(actual_result.get("data", [])[:50]),  # Limit rows
            actual_row_count=actual_result.get("row_count", 0),
        )

    @abstractmethod
    def _call_llm(self, prompt: str) -> str:
        """Call the LLM API. Implemented by subclasses."""
        pass

    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response into structured result."""
        try:
            # Try to extract JSON from response
            # Handle markdown code blocks
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                response = response[start:end].strip()
            elif "```" in response:
                start = response.find("```") + 3
                end = response.find("```", start)
                response = response[start:end].strip()

            result = json.loads(response)

            return {
                "passed": result.get("passed", False),
                "confidence": result.get("confidence", 0.5),
                "failure_reason": result.get("failure_reason"),
                "failure_category": result.get("failure_category"),
                "explanation": result.get("explanation", "")
            }
        except json.JSONDecodeError:
            # Fallback: try to infer from text
            passed = "passed" in response.lower() and "true" in response.lower()
            return {
                "passed": passed,
                "confidence": 0.5,
                "failure_reason": "Failed to parse LLM response" if not passed else None,
                "failure_category": "other" if not passed else None,
                "explanation": response[:500]
            }


class AnthropicJudge(LLMJudge):
    """Judge using Anthropic Claude API."""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "claude-3-5-sonnet-20241022"
    ):
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not provided")

        super().__init__(model=model)

        import anthropic
        self._client = anthropic.Anthropic(api_key=self.api_key)

    def _call_llm(self, prompt: str) -> str:
        """Call Claude API."""
        response = self._client.messages.create(
            model=self.model,
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text


class OpenAIJudge(LLMJudge):
    """Judge using OpenAI GPT API."""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4-turbo-preview"
    ):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not provided")

        super().__init__(model=model)

        import openai
        self._client = openai.OpenAI(api_key=self.api_key)

    def _call_llm(self, prompt: str) -> str:
        """Call OpenAI API."""
        response = self._client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024
        )
        return response.choices[0].message.content
